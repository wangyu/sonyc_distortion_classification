{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lizichen/Downloads\n"
     ]
    }
   ],
   "source": [
    "cd /home/lizichen/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizichen/anaconda3/envs/modal/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime\n",
    "import csv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File('features.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_recording = list(f.values())[0]\n",
    "cluster_recording.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Peeking: The first ten 3-seconds audio clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_recording[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_recording[1][1].decode(\"utf-8\").split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = cluster_recording[1][2]\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert the plain data format to dictionary for fast key look-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldata = collections.defaultdict(dict)\n",
    "\n",
    "numberOfDataPoints = cluster_recording.shape[0]\n",
    "\n",
    "counter = 0\n",
    "print('Start: '+str(datetime.datetime.now()))\n",
    "\n",
    "for datapoint in range(numberOfDataPoints):\n",
    "    sensor_and_timestamp = cluster_recording[datapoint][1].decode(\"utf-8\")\n",
    "    if len(sensor_and_timestamp.split('_')) == 2:\n",
    "        sensor_id = cluster_recording[datapoint][1].decode(\"utf-8\").split('_')[0]\n",
    "        timestamp = cluster_recording[datapoint][1].decode(\"utf-8\").split('_')[1]\n",
    "        embedding = cluster_recording[datapoint][2]\n",
    "        alldata[sensor_id][timestamp] = embedding\n",
    "    elif len(sensor_and_timestamp.split('-')) == 2:\n",
    "        sensor_id = cluster_recording[datapoint][1].decode(\"utf-8\").split('-')[0]\n",
    "        timestamp = cluster_recording[datapoint][1].decode(\"utf-8\").split('-')[1]\n",
    "        embedding = cluster_recording[datapoint][2]\n",
    "        alldata[sensor_id][timestamp] = embedding\n",
    "    else:\n",
    "        print('Warning: Not splittable!:' + sensor_and_timestamp)\n",
    "    \n",
    "    counter += 1\n",
    "        \n",
    "    if counter % 150000 == 0:\n",
    "        print(str(counter) + ' ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Save the re-formatted feature data; now in the format of DefaultDictionary, into pickle file for efficient reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"features.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(alldata, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load saved dictionary pickle! (All features data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alldata_dict = collections.defaultdict(dict)\n",
    "with (open(\"features.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            alldata_dict= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Example: Now we have all the sensor IDs and timestamp effectively accessible in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['74da385c6855', 'b827ebb40450', 'b827eb4cc22e', 'b827eb84deb5', 'b827eb44506f', 'b827eb0d8af7', 'b827eb7b2c3e', 'b827eb252949', 'b827eb977bfb', 'b827ebe1fe4b', 'b827eb9bed23', 'b827eb5d1714', 'b827eb539980', '0013ef2008e6', 'b827eb2a1bce', 'b827eb3bda47', 'b827eb86d458', 'b827ebad073b', 'b827eb29eb77', 'b827eb820cfe', '0013ef6b0a0f', 'b827eb2c65db', 'b827ebfd616c', 'b827eb8e2420', 'b827eb905497', '74da385c684f', '0013ef700444', 'b827eb122f0f', 'b827eb32f75c', 'b827eb429cd4', '74da385c683d', 'b827ebba613d', 'b827eb3e842e', '0013ef801412', 'b827eb0fedda', 'b827eb5895e9', 'b827ebc7f772', 'b827ebf9d204', 'b827eb73e772', '74da385c687d', 'b827eb4e7821', 'b827eb9d0e7f', '74da383b5ca4', 'b827eb1685c7', 'b827eb815321', 'b827ebefb215', 'b827eb43d8f4', 'b827ebdd5c38', 'b827eb8e1f0b', 'b827eb8e32ad', 'b827eb132382', 'b827eb42bd4a', 'b827ebc6dcc6', 'b827eb329ab8', 'b827ebf31214'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1484629333.39', '1486275276.49', '1486270823.64', '1484456462.61', '1484506256.94', '1484456868.87', '1485062169.07', '1485061248.92', '1485107944.57', '1485105300.84', '1484370845.88', '1484370025.38', '1484377939.68', '1484377520.33', '1484802567.76', '1484812312.72', '1484802064.4', '1484811876.07', '1484715649.85', '1485320440.89', '1485397215.11', '1486457713.33', '1486510104.53', '1486509851.17', '1485493226.56', '1485982646.26', '1484888890.45', '1484888476.2', '1484974865.71', '1485255027.89', '1485241169.45', '1485249454.09'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_dict['74da385c6855'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read the positive_samples.pickle file - all positive sensor_id and timestamp\n",
    "positive_samples.pickle file from Yu Wang, April 6, 2018, shared on Slack group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alldata_dict = collections.defaultdict(dict)\n",
    "with (open(\"positive_samples.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            positive_samples= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b827eb429cd4': ['1488836836.42', '1488810598.15'],\n",
       " 'b827eb5895e9': ['1490690898.85', '1490686108.53'],\n",
       " 'b827eb0d8af7': ['1485198017.91', '1485163671.07'],\n",
       " 'b827eb122f0f': ['1498892374.76', '1498261062.11'],\n",
       " 'b827eb86d458': ['1487000875.51', '1487028598.04'],\n",
       " 'b827eb0fedda': ['1497928649.89', '1480740512.58'],\n",
       " 'b827eb8e2420': ['1500149760.47', '1500173615.97'],\n",
       " 'b827eb1685c7': ['1499724045.58', '1486212620.04'],\n",
       " 'b827eb815321': ['1482308965.46', '1482399908.63'],\n",
       " 'b827eb9bed23': ['1491297344.73', '1491323380.55'],\n",
       " 'b827eb44506f': ['1486149582.98', '1499473078.4'],\n",
       " 'b827eb4e7821': ['1483014176.71', '1483015339.86'],\n",
       " 'b827eb2a1bce': ['1487029980.37', '1487018417.81'],\n",
       " 'b827ebad073b': ['1480303515.84', '1480297488.98'],\n",
       " 'b827eb42bd4a': ['1495845294.38', '1495820507.31']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Select the positive embeddings according to the csv file data  - Set up positive training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalPositiveEmbedding = np.empty([1,128], dtype=int)\n",
    "for sensorid in positive_samples.keys():\n",
    "    for timestamp in positive_samples[sensorid]:\n",
    "        if sensorid in alldata_dict.keys():\n",
    "            if timestamp in alldata_dict[sensorid].keys():\n",
    "                embedding = alldata_dict[sensorid][timestamp]\n",
    "                totalPositiveEmbedding = np.vstack((totalPositiveEmbedding, embedding))\n",
    "        \n",
    "totalPositiveEmbedding =np.delete(totalPositiveEmbedding, 0, 0) # remove the first dummy 128 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalPositiveEmbedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the positive label, 1\n",
    "positive_xy = np.insert(totalPositiveEmbedding, 128, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 129)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Save the positive training embedding data to pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"positive_xy.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(positive_xy, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load the pickle! (positive training numpy object 170 $\\times$ (128+1), with 1 mark as Is-Noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with (open(\"positive_xy.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            positive_xy= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read the negative csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negativeCsvData = {}\n",
    "with open('Sensor_data_negative.csv') as csvfile:\n",
    "    Sensor_data_positive = csv.reader(csvfile, skipinitialspace=True, delimiter=' ')\n",
    "    csvrow = -1\n",
    "    for row in Sensor_data_positive:\n",
    "        if csvrow != -1: # skip the first row\n",
    "            sid = row[0].split(',')[0]\n",
    "            ts  = row[0].split(',')[1]\n",
    "            if sid in negativeCsvData.keys():\n",
    "                negativeCsvData[sid].append(ts)\n",
    "            else:\n",
    "                negativeCsvData[sid] = []\n",
    "                negativeCsvData[sid].append(ts)\n",
    "        csvrow += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "15\n",
      "['1491211362.82_5', '1491800203.12_8', '1482424939.17_1', '1498868810.69_4', '1488993967.2_1', '1492066899.36_6', '1484297539.06_6', '1490406200.26_1', '1494789341.92_4', '1490926970.82_4', '1488422336.29_2', '1488377458.61_6', '1486479347.85_4', '1483636861.26_6', '1484122883.92_3', '1487404654.04_8', '1492186534.34_2', '1493876183.72_2', '1483134229.53_1', '1491421469.99_7']\n"
     ]
    }
   ],
   "source": [
    "print(type(negativeCsvData))\n",
    "print(len(negativeCsvData))\n",
    "print(negativeCsvData['b827eb4e7821'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pull negative embeddings according to the negative sensor and timestamp data (negativeCsvData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalNegativeEmbedding = np.empty([1,128], dtype=int)\n",
    "for sensorid in negativeCsvData.keys():\n",
    "    for timestamp in negativeCsvData[sensorid]:\n",
    "        if sensorid in alldata_dict.keys():\n",
    "            all_timestamp_full = negativeCsvData[sensorid]\n",
    "            for timestamp_full in all_timestamp_full:\n",
    "                timestamp = timestamp_full.split('_')[0]\n",
    "                embedding_index = timestamp_full.split('_')[1]\n",
    "                if timestamp in alldata_dict[sensorid]:\n",
    "                    one_negative_embedding = alldata_dict[sensorid][timestamp][int(embedding_index)]\n",
    "                    totalNegativeEmbedding = np.vstack((totalNegativeEmbedding, one_negative_embedding))\n",
    "            \n",
    "totalNegativeEmbedding =np.delete(totalNegativeEmbedding, 0, 0) # remove the first dummy 128 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3980, 128)\n",
      "[[146  14 129 ...   0  62 255]\n",
      " [154  30 171 ...  79 158 255]\n",
      " [153  25 150 ... 114 150 255]\n",
      " ...\n",
      " [164  33 165 ... 211 244 255]\n",
      " [164  37 180 ... 174 166 255]\n",
      " [153  28 167 ...  76 108 255]]\n"
     ]
    }
   ],
   "source": [
    "print(totalNegativeEmbedding.shape)\n",
    "print(totalNegativeEmbedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Add negative labels (0) to the negative embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the negative label, 0\n",
    "negative_xy = np.insert(totalNegativeEmbedding, 128, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3980, 129)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Save the negative embedding and labels to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"negative_xy.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(negative_xy, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Load the negative_xy pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with (open(\"negative_xy.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            negative_xy= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Summary:\n",
    "Now we have the following datasets that have been cleaned and formatted to pickles for better usage:  \n",
    "1. ``features.pickle``\n",
    "2. ``positive_xy.pickle``\n",
    "3. ``negative_xy.pickle``\n",
    "\n",
    "The original raw data files are:\n",
    "1. ``features.h5``\n",
    "2. ``positive_samples.pickle``\n",
    "3. ``Sensor_data_negative.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modal]",
   "language": "python",
   "name": "conda-env-modal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
