{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lizichen/Downloads\n"
     ]
    }
   ],
   "source": [
    "cd /home/lizichen/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizichen/anaconda3/envs/modal/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File('features.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5036582,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_recording = list(f.values())[0]\n",
    "cluster_recording.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first ten 3-seconds audio clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/scratch/mc6591/sonyc/features/sonycnode-b827ebb40450.sonyc/2017-08-26.hdf5'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_recording[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b827ebb40450', '1503787235.74']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_recording[1][1].decode(\"utf-8\").split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[152,  27, 156, ..., 110, 105, 255],\n",
       "       [150,  22, 137, ...,  80, 102, 255],\n",
       "       [152,  26, 150, ...,  48,  62, 255],\n",
       "       ...,\n",
       "       [153,  27, 151, ...,  44,  34, 255],\n",
       "       [151,  25, 145, ...,   0,   0, 255],\n",
       "       [147,  14, 123, ...,   0,   0, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = cluster_recording[1][2]\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the plain data format to dictionary for fast key look-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-04-05 19:49:30.961912\n",
      "150000 2018-04-05 19:50:27.908152\n",
      "300000 2018-04-05 19:51:24.199239\n",
      "450000 2018-04-05 19:52:20.426115\n",
      "600000 2018-04-05 19:53:16.737594\n",
      "750000 2018-04-05 19:54:13.210527\n",
      "900000 2018-04-05 19:55:09.444046\n",
      "1050000 2018-04-05 19:56:05.597641\n",
      "1200000 2018-04-05 19:57:01.906817\n",
      "1350000 2018-04-05 19:57:58.204949\n",
      "1500000 2018-04-05 19:58:54.372835\n",
      "1650000 2018-04-05 19:59:50.545104\n",
      "1800000 2018-04-05 20:00:46.760075\n",
      "1950000 2018-04-05 20:01:43.248187\n",
      "2100000 2018-04-05 20:02:39.488306\n",
      "2250000 2018-04-05 20:03:36.033967\n",
      "2400000 2018-04-05 20:04:32.278429\n",
      "2550000 2018-04-05 20:05:26.862019\n",
      "2700000 2018-04-05 20:06:20.601055\n",
      "2850000 2018-04-05 20:07:14.365796\n",
      "3000000 2018-04-05 20:08:08.171276\n",
      "3150000 2018-04-05 20:09:01.269701\n",
      "3300000 2018-04-05 20:09:54.264548\n",
      "3450000 2018-04-05 20:10:47.230346\n",
      "3600000 2018-04-05 20:11:41.704015\n",
      "3750000 2018-04-05 20:12:37.680491\n",
      "3900000 2018-04-05 20:13:35.587818\n",
      "4050000 2018-04-05 20:14:34.263256\n",
      "4200000 2018-04-05 20:15:33.135489\n",
      "4350000 2018-04-05 20:16:31.918865\n",
      "4500000 2018-04-05 20:17:31.002479\n",
      "4650000 2018-04-05 20:18:29.475518\n",
      "4800000 2018-04-05 20:19:28.835237\n",
      "4950000 2018-04-05 20:20:28.052908\n"
     ]
    }
   ],
   "source": [
    "alldata = collections.defaultdict(dict)\n",
    "\n",
    "numberOfDataPoints = cluster_recording.shape[0]\n",
    "\n",
    "counter = 0\n",
    "print('Start: '+str(datetime.datetime.now()))\n",
    "\n",
    "for datapoint in range(numberOfDataPoints):\n",
    "    sensor_and_timestamp = cluster_recording[datapoint][1].decode(\"utf-8\")\n",
    "    if len(sensor_and_timestamp.split('_')) == 2:\n",
    "        sensor_id = cluster_recording[datapoint][1].decode(\"utf-8\").split('_')[0]\n",
    "        timestamp = cluster_recording[datapoint][1].decode(\"utf-8\").split('_')[1]\n",
    "        embedding = cluster_recording[datapoint][2]\n",
    "        alldata[sensor_id][timestamp] = embedding\n",
    "    elif len(sensor_and_timestamp.split('-')) == 2:\n",
    "        sensor_id = cluster_recording[datapoint][1].decode(\"utf-8\").split('-')[0]\n",
    "        timestamp = cluster_recording[datapoint][1].decode(\"utf-8\").split('-')[1]\n",
    "        embedding = cluster_recording[datapoint][2]\n",
    "        alldata[sensor_id][timestamp] = embedding\n",
    "    else:\n",
    "        print('Warning: Not splittable!:' + sensor_and_timestamp)\n",
    "    \n",
    "    counter += 1\n",
    "        \n",
    "    if counter % 150000 == 0:\n",
    "        print(str(counter) + ' ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"features.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(alldata, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved dictionary pickle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alldata_dict = collections.defaultdict(dict)\n",
    "with (open(\"features.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            alldata_dict= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have all the sensor IDs and timestamp effectively accessible in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['74da385c6855', 'b827ebb40450', 'b827eb4cc22e', 'b827eb84deb5', 'b827eb44506f', 'b827eb0d8af7', 'b827eb7b2c3e', 'b827eb252949', 'b827eb977bfb', 'b827ebe1fe4b', 'b827eb9bed23', 'b827eb5d1714', 'b827eb539980', '0013ef2008e6', 'b827eb2a1bce', 'b827eb3bda47', 'b827eb86d458', 'b827ebad073b', 'b827eb29eb77', 'b827eb820cfe', '0013ef6b0a0f', 'b827eb2c65db', 'b827ebfd616c', 'b827eb8e2420', 'b827eb905497', '74da385c684f', '0013ef700444', 'b827eb122f0f', 'b827eb32f75c', 'b827eb429cd4', '74da385c683d', 'b827ebba613d', 'b827eb3e842e', '0013ef801412', 'b827eb0fedda', 'b827eb5895e9', 'b827ebc7f772', 'b827ebf9d204', 'b827eb73e772', '74da385c687d', 'b827eb4e7821', 'b827eb9d0e7f', '74da383b5ca4', 'b827eb1685c7', 'b827eb815321', 'b827ebefb215', 'b827eb43d8f4', 'b827ebdd5c38', 'b827eb8e1f0b', 'b827eb8e32ad', 'b827eb132382', 'b827eb42bd4a', 'b827ebc6dcc6', 'b827eb329ab8', 'b827ebf31214'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1484629333.39', '1486275276.49', '1486270823.64', '1484456462.61', '1484506256.94', '1484456868.87', '1485062169.07', '1485061248.92', '1485107944.57', '1485105300.84', '1484370845.88', '1484370025.38', '1484377939.68', '1484377520.33', '1484802567.76', '1484812312.72', '1484802064.4', '1484811876.07', '1484715649.85', '1485320440.89', '1485397215.11', '1486457713.33', '1486510104.53', '1486509851.17', '1485493226.56', '1485982646.26', '1484888890.45', '1484888476.2', '1484974865.71', '1485255027.89', '1485241169.45', '1485249454.09'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_dict['74da385c6855'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now read the positive csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvdata = {}\n",
    "with open('Sensor_data_positive.csv') as csvfile:\n",
    "    Sensor_data_positive = csv.reader(csvfile, skipinitialspace=True, delimiter=' ')\n",
    "    csvrow = -1\n",
    "    for row in Sensor_data_positive:\n",
    "        if csvrow != -1: # skip the first row\n",
    "            sid = row[0].split(',')[0]\n",
    "            ts  = row[0].split(',')[1].split('_')[0]\n",
    "            if sid in csvdata.keys():\n",
    "                csvdata[sid].append(ts)\n",
    "            else:\n",
    "                csvdata[sid] = []\n",
    "                csvdata[sid].append(ts)\n",
    "        csvrow += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the positive embeddings according to the csv file data  - Set up training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalPositiveEmbedding = np.empty([1,128], dtype=int)\n",
    "\n",
    "for sensor in csvdata:\n",
    "    for tt in csvdata[sensor]:\n",
    "        if tt in alldata_dict[sensor].keys():\n",
    "            embedding = alldata_dict[sensor][tt]\n",
    "            totalPositiveEmbedding = np.vstack((totalPositiveEmbedding, embedding))\n",
    "\n",
    "totalPositiveEmbedding =np.delete(totalPositiveEmbedding, 0, 0) # remove the first dummy 128 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "910"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfPositiveEmbedding = totalPositiveEmbedding.shape[0]\n",
    "numberOfPositiveEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_xy = np.insert(totalPositiveEmbedding, 128, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149,  25, 123, ...,   0, 255,   1],\n",
       "       [147,  25, 114, ...,   0, 255,   1],\n",
       "       [148,  26, 117, ...,   0, 255,   1],\n",
       "       ...,\n",
       "       [153,  16, 149, ..., 148, 255,   1],\n",
       "       [154,  18, 133, ..., 150, 255,   1],\n",
       "       [153,  19, 141, ..., 255, 255,   1]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the positive training data to pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"positive_xy.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(positive_xy, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: a positive labeled sensor ID: b827eb44506f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5036582):\n",
    "    sensor_time = str(cluster_recording[i][1])\n",
    "    if 'b827eb44506f' in sensor_time and '1499290501.28' in sensor_time:\n",
    "        print('found')\n",
    "        f1 = cluster_recording[i][0]\n",
    "        f2 = cluster_recording[i][1]\n",
    "        emb = cluster_recording[i][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given Ana's Excel sheet, pull out all positive (noise) data - Sensor_data_positive.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modal]",
   "language": "python",
   "name": "conda-env-modal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
