{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lizichen1/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime\n",
    "import csv\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 13:14:23) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('features.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5036582,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_recording = list(f.values())[0]\n",
    "cluster_recording.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Peeking: The first ten 3-seconds audio clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/scratch/mc6591/sonyc/features/sonycnode-b827ebb40450.sonyc/2017-08-26.hdf5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_recording[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b827ebb40450', '1503787235.74']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_recording[1][1].decode(\"utf-8\").split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[152,  27, 156, ..., 110, 105, 255],\n",
       "       [150,  22, 137, ...,  80, 102, 255],\n",
       "       [152,  26, 150, ...,  48,  62, 255],\n",
       "       ..., \n",
       "       [153,  27, 151, ...,  44,  34, 255],\n",
       "       [151,  25, 145, ...,   0,   0, 255],\n",
       "       [147,  14, 123, ...,   0,   0, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = cluster_recording[1][2]\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert the plain data format to dictionary for fast key look-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-04-15 14:29:20.328806\n",
      "150000 2018-04-15 14:30:30.882628\n",
      "300000 2018-04-15 14:31:41.793707\n",
      "450000 2018-04-15 14:32:52.866983\n",
      "600000 2018-04-15 14:34:03.708763\n",
      "750000 2018-04-15 14:35:14.716887\n",
      "900000 2018-04-15 14:36:25.763546\n",
      "1050000 2018-04-15 14:37:36.449837\n",
      "1200000 2018-04-15 14:38:46.566283\n",
      "1350000 2018-04-15 14:39:57.197749\n",
      "1500000 2018-04-15 14:41:07.087242\n",
      "1650000 2018-04-15 14:42:17.441425\n",
      "1800000 2018-04-15 14:43:29.261708\n",
      "1950000 2018-04-15 14:44:40.380015\n",
      "2100000 2018-04-15 14:45:50.351543\n",
      "2250000 2018-04-15 14:47:00.075252\n",
      "2400000 2018-04-15 14:48:09.823971\n",
      "2550000 2018-04-15 14:49:19.412391\n",
      "2700000 2018-04-15 14:50:28.949616\n",
      "2850000 2018-04-15 14:51:37.994237\n",
      "3000000 2018-04-15 14:52:48.207530\n",
      "3150000 2018-04-15 14:53:58.567916\n",
      "3300000 2018-04-15 14:55:08.085415\n",
      "3450000 2018-04-15 14:56:17.653236\n",
      "3600000 2018-04-15 14:57:31.433095\n",
      "3750000 2018-04-15 14:58:43.666232\n",
      "3900000 2018-04-15 14:59:53.731985\n",
      "4050000 2018-04-15 15:01:03.796854\n",
      "4200000 2018-04-15 15:02:13.477742\n",
      "4350000 2018-04-15 15:03:23.211270\n",
      "4500000 2018-04-15 15:04:32.673475\n",
      "4650000 2018-04-15 15:05:42.345685\n",
      "4800000 2018-04-15 15:06:52.129615\n",
      "4950000 2018-04-15 15:08:01.666510\n"
     ]
    }
   ],
   "source": [
    "alldata = collections.defaultdict(dict)\n",
    "\n",
    "numberOfDataPoints = cluster_recording.shape[0]\n",
    "\n",
    "counter = 0\n",
    "print('Start: '+str(datetime.datetime.now()))\n",
    "\n",
    "for datapoint in range(numberOfDataPoints):\n",
    "    sensor_and_timestamp = cluster_recording[datapoint][1].decode(\"utf-8\")\n",
    "    if len(sensor_and_timestamp.split('_')) == 2:\n",
    "        sensor_id = cluster_recording[datapoint][1].decode(\"utf-8\").split('_')[0]\n",
    "        timestamp = cluster_recording[datapoint][1].decode(\"utf-8\").split('_')[1]\n",
    "        embedding = cluster_recording[datapoint][2]\n",
    "        alldata[sensor_id][timestamp] = embedding\n",
    "    elif len(sensor_and_timestamp.split('-')) == 2:\n",
    "        sensor_id = cluster_recording[datapoint][1].decode(\"utf-8\").split('-')[0]\n",
    "        timestamp = cluster_recording[datapoint][1].decode(\"utf-8\").split('-')[1]\n",
    "        embedding = cluster_recording[datapoint][2]\n",
    "        alldata[sensor_id][timestamp] = embedding\n",
    "    else:\n",
    "        print('Warning: Not splittable!:' + sensor_and_timestamp)\n",
    "    \n",
    "    counter += 1\n",
    "        \n",
    "    if counter % 150000 == 0:\n",
    "        print(str(counter) + ' ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Save the re-formatted feature data; now in the format of DefaultDictionary, into pickle file for efficient reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(alldata, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load saved dictionary pickle! (All features data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed\n"
     ]
    }
   ],
   "source": [
    "# alldata_dict = collections.defaultdict(dict)\n",
    "with (open(\"features.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            alldata_dict= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            print('failed')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Example: Now we have all the sensor IDs and timestamp effectively accessible in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alldata_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9aadeb0ff101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malldata_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'alldata_dict' is not defined"
     ]
    }
   ],
   "source": [
    "alldata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1484629333.39', '1486275276.49', '1486270823.64', '1484456462.61', '1484506256.94', '1484456868.87', '1485062169.07', '1485061248.92', '1485107944.57', '1485105300.84', '1484370845.88', '1484370025.38', '1484377939.68', '1484377520.33', '1484802567.76', '1484812312.72', '1484802064.4', '1484811876.07', '1484715649.85', '1485320440.89', '1485397215.11', '1486457713.33', '1486510104.53', '1486509851.17', '1485493226.56', '1485982646.26', '1484888890.45', '1484888476.2', '1484974865.71', '1485255027.89', '1485241169.45', '1485249454.09'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata_dict['74da385c6855'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read the positive_samples.pickle file - all positive sensor_id and timestamp\n",
    "positive_samples.pickle file from Yu Wang, April 6, 2018, shared on Slack group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alldata_dict = collections.defaultdict(dict)\n",
    "with (open(\"positive_samples.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            positive_samples= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b827eb429cd4': ['1488836836.42', '1488810598.15'],\n",
       " 'b827eb5895e9': ['1490690898.85', '1490686108.53'],\n",
       " 'b827eb0d8af7': ['1485198017.91', '1485163671.07'],\n",
       " 'b827eb122f0f': ['1498892374.76', '1498261062.11'],\n",
       " 'b827eb86d458': ['1487000875.51', '1487028598.04'],\n",
       " 'b827eb0fedda': ['1497928649.89', '1480740512.58'],\n",
       " 'b827eb8e2420': ['1500149760.47', '1500173615.97'],\n",
       " 'b827eb1685c7': ['1499724045.58', '1486212620.04'],\n",
       " 'b827eb815321': ['1482308965.46', '1482399908.63'],\n",
       " 'b827eb9bed23': ['1491297344.73', '1491323380.55'],\n",
       " 'b827eb44506f': ['1486149582.98', '1499473078.4'],\n",
       " 'b827eb4e7821': ['1483014176.71', '1483015339.86'],\n",
       " 'b827eb2a1bce': ['1487029980.37', '1487018417.81'],\n",
       " 'b827ebad073b': ['1480303515.84', '1480297488.98'],\n",
       " 'b827eb42bd4a': ['1495845294.38', '1495820507.31']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Select the positive embeddings according to the csv file data  - Set up positive training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalPositiveEmbedding = np.empty([1,128], dtype=int)\n",
    "for sensorid in positive_samples.keys():\n",
    "    for timestamp in positive_samples[sensorid]:\n",
    "        if sensorid in alldata_dict.keys():\n",
    "            if timestamp in alldata_dict[sensorid].keys():\n",
    "                embedding = alldata_dict[sensorid][timestamp]\n",
    "                totalPositiveEmbedding = np.vstack((totalPositiveEmbedding, embedding))\n",
    "        \n",
    "totalPositiveEmbedding =np.delete(totalPositiveEmbedding, 0, 0) # remove the first dummy 128 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalPositiveEmbedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the positive label, 1\n",
    "positive_xy = np.insert(totalPositiveEmbedding, 128, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 129)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Save the positive training embedding data to pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"positive_xy.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(positive_xy, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load the pickle! (positive training numpy object 170 $\\times$ (128+1), with 1 mark as Is-Noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with (open(\"positive_xy.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            positive_xy= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read the negative csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                          \u001b[1m\u001b[34mmy_code_NO_COMMIT\u001b[m\u001b[m/\r\n",
      "Sensor_data_negative.csv           negative_xy.pickle\r\n",
      "choosing_sensors.ipynb             \u001b[1m\u001b[34mpapers\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34mcode\u001b[m\u001b[m/                              positive_samples.pickle\r\n",
      "\u001b[1m\u001b[34mgoogle_active-learning_githubrepo\u001b[m\u001b[m/ positive_xy.pickle\r\n",
      "\u001b[1m\u001b[34mmodal-playground\u001b[m\u001b[m/                  positive_xy_wrong.pickle\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negativeCsvData = {}\n",
    "with open('Sensor_data_negative.csv') as csvfile:\n",
    "    Sensor_data_positive = csv.reader(csvfile, skipinitialspace=True, delimiter=' ')\n",
    "    csvrow = -1\n",
    "    for row in Sensor_data_positive:\n",
    "        if csvrow != -1: # skip the first row\n",
    "            sid = row[0].split(',')[0]\n",
    "            ts  = row[0].split(',')[1]\n",
    "            if sid in negativeCsvData.keys():\n",
    "                negativeCsvData[sid].append(ts)\n",
    "            else:\n",
    "                negativeCsvData[sid] = []\n",
    "                negativeCsvData[sid].append(ts)\n",
    "        csvrow += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "15\n",
      "['1491211362.82_5', '1491800203.12_8', '1482424939.17_1', '1498868810.69_4', '1488993967.2_1', '1492066899.36_6', '1484297539.06_6', '1490406200.26_1', '1494789341.92_4', '1490926970.82_4', '1488422336.29_2', '1488377458.61_6', '1486479347.85_4', '1483636861.26_6', '1484122883.92_3', '1487404654.04_8', '1492186534.34_2', '1493876183.72_2', '1483134229.53_1', '1491421469.99_7']\n"
     ]
    }
   ],
   "source": [
    "print(type(negativeCsvData))\n",
    "print(len(negativeCsvData))\n",
    "print(negativeCsvData['b827eb4e7821'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1491211362.82_5', '1491800203.12_8', '1482424939.17_1', '1498868810.69_4', '1488993967.2_1', '1492066899.36_6', '1484297539.06_6', '1490406200.26_1', '1494789341.92_4', '1490926970.82_4', '1488422336.29_2', '1488377458.61_6', '1486479347.85_4', '1483636861.26_6', '1484122883.92_3', '1487404654.04_8', '1492186534.34_2', '1493876183.72_2', '1483134229.53_1', '1491421469.99_7']\n",
      "['1495523629.05_6', '1480117895.75_1', '1488292369.26_8', '1492102763.57_1', '1480694468.75_4', '1493213984.51_3', '1491487510.36_4', '1464325406.85_4', '1492035667.77_7', '1488896319.7_5', '1495720875.77_4', '1482091712.59_8', '1464240195.4_6', '1479310178.0_3', '1496909426.89_4', '1482329266.43_4', '1479682002.65_8', '1479267519.49_5', '1487318170.88_4', '1482296730.24_5']\n",
      "['1492046254.64_1', '1488310935.14_4', '1497759001.33_2', '1495153300.1_3', '1481478245.04_4', '1489760860.19_5', '1479413956.79_5', '1481412339.02_4', '1485885766.44_4', '1493425976.33_4', '1494547745.44_8', '1497484597.2_3', '1500126760.74_8', '1498217189.68_7', '1498398029.51_2', '1482351393.66_3', '1495905017.16_8', '1488091729.93_5', '1480656779.11_2', '1486847844.97_3']\n",
      "['1482705118.71_8', '1492908659.59_8', '1495794613.18_5', '1484104702.54_1', '1495557439.53_2', '1499571191.92_4', '1494814605.05_6', '1489583804.6_5', '1482931237.86_3', '1482865013.77_2', '1488039953.23_2', '1498995372.57_1', '1491305642.07_6', '1484425684.32_1', '1485100730.88_1', '1493087495.12_5', '1497527749.43_6', '1493351683.17_4', '1489591793.01_2', '1490396078.48_7']\n",
      "['1489705575.55_5', '1497083157.66_7', '1481012535.01_7', '1493879205.95_3', '1483698702.64_7', '1499515739.92_5', '1499425146.75_3', '1483036280.55_6', '1493007014.82_1', '1483139625.7_4', '1491996575.98_2', '1496231481.08_4', '1479632051.31_1', '1494549298.01_8', '1482638594.22_2', '1495567196.43_3', '1491602501.38_5', '1491921453.71_6', '1482958508.4_6', '1499444107.9_7']\n",
      "['1487770296.06_3', '1499302701.64_1', '1488641176.69_7', '1488539365.43_6', '1499818724.82_1', '1485585675.18_1', '1482368024.69_7', '1496361356.06_6', '1493386627.83_2', '1492202795.5_4', '1499134766.31_3', '1495743301.39_3', '1487816645.9_3', '1499502281.6_5', '1498701411.62_4', '1488943479.86_7', '1490787850.3_1', '1499388062.29_8', '1488151527.44_7', '1488047436.01_3']\n",
      "['1495200967.99_8', '1484364896.42_5', '1491770775.55_1', '1488885560.64_4', '1497961310.96_5', '1488590006.99_1', '1487299695.07_5', '1487396419.6_4', '1481347209.88_1', '1485555378.33_4', '1498506230.98_8', '1496273425.41_8', '1464286501.93_4', '1498048737.89_8', '1495039371.39_2', '1493720709.4_4', '1499875924.18_4', '1481166168.31_5', '1496539042.04_3', '1491630422.35_3']\n",
      "['1499613017.17_1', '1499528235.09_1', '1491847975.78_2', '1499731977.84_2', '1480112906.87_1', '1488900052.8_2', '1482943771.26_3', '1494187463.05_4', '1480410212.37_6', '1486162016.89_1', '1495149853.94_5', '1487458226.96_8', '1499552726.43_6', '1488236835.88_5', '1484192267.86_8', '1484296068.44_6', '1493241027.83_5', '1464209524.38_5', '1487327465.82_7', '1492279425.74_1']\n",
      "['1497490137.97_5', '1494590852.18_5', '1495921446.63_7', '1497984542.12_8', '1497048532.47_5', '1494850432.99_1', '1492927028.08_6', '1493077890.59_1', '1490280041.15_1', '1499192554.11_3', '1493595378.55_3', '1493234818.18_5', '1495306430.93_3', '1494567409.92_8', '1497430997.82_6', '1490060542.09_1', '1490060542.09_2', '1490060542.09_3', '1490060542.09_4', '1490060542.09_5']\n",
      "['1487698943.28_5', '1498946997.51_1', '1484592410.76_5', '1486742249.47_2', '1488716643.64_1', '1491043468.64_2', '1479670751.58_4', '1489077760.73_4', '1497563780.38_3', '1492410662.23_8', '1499038527.25_5', '1495104581.09_6', '1480373867.17_5', '1489393416.0_4', '1496325662.98_6', '1484782212.37_2', '1493388050.73_8', '1497060279.02_6', '1478957725.14_5', '1497594742.92_3']\n",
      "['1482608459.35_2', '1491367798.51_5', '1491648138.22_1', '1491905562.66_5', '1494415148.94_7', '1482713009.16_8', '1483089944.44_8', '1487416206.77_2', '1482530842.36_4', '1479553279.5_2', '1479748045.8_6', '1480963111.09_6', '1488428766.76_2', '1488596776.32_2', '1486446995.46_3', '1479325108.22_8', '1492922865.71_6', '1494215534.17_1', '1487464657.03_7', '1490594229.13_3']\n",
      "['1485827670.09_2', '1495659850.31_7', '1488110454.92_4', '1488763610.66_3', '1496132284.56_8', '1482011685.79_6', '1484777443.4_8', '1498190503.91_3', '1498881000.86_4', '1486342331.37_4', '1496896033.56_1', '1496904711.38_7', '1493963224.14_1', '1484557041.57_1', '1490000398.26_6', '1482182335.84_2', '1482293568.4_2', '1481075858.69_3', '1483419342.79_6', '1484952395.62_3']\n",
      "['1496584193.07_2', '1492451796.39_3', '1493218688.66_8', '1492228390.03_8', '1490604758.18_6', '1490688206.43_8', '1493580020.57_5', '1488063988.26_2', '1498488196.25_3', '1492426725.42_5', '1493677133.82_2', '1495925319.59_5', '1499090357.93_7', '1494867086.62_5', '1493564113.69_2', '1492723231.0_1', '1492278122.53_4', '1494336954.84_2', '1496050768.5_4', '1492684180.92_4']\n",
      "['1481123928.9_1', '1485559141.89_3', '1481276857.05_6', '1484184158.78_5', '1495673910.82_4', '1479150242.71_3', '1487971162.63_2', '1498460119.33_4', '1492314821.66_2', '1480817626.1_4', '1500290776.12_3', '1483623224.7_3', '1484070700.58_2', '1479152523.2_3', '1495276586.87_2', '1498340225.51_5', '1498340225.51_5', '1492870851.41_6', '1498464532.0_7', '1487835532.73_5']\n",
      "['1482464662.24_5', '1499786219.47_8', '1491963104.37_6', '1498802119.0_4', '1495912027.23_7', '1495024822.71_8', '1492997943.91_2', '1492312349.57_5', '1495145824.62_4', '1497006724.91_4', '1482642816.37_2', '1493320149.64_2', '1497905384.37_1', '1489297648.18_5', '1500121936.51_6', '1496026518.61_3', '1498238952.91_2', '1495680055.31_8', '1499458546.46_6', '1496413283.32_2']\n"
     ]
    }
   ],
   "source": [
    "negativeCount = 0\n",
    "for key in negativeCsvData.keys():\n",
    "    timestamps = negativeCsvData[key]\n",
    "    print(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pull negative embeddings according to the negative sensor and timestamp data (negativeCsvData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alldata_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b74182f1437f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msensorid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegativeCsvData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegativeCsvData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msensorid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msensorid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malldata_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mall_timestamp_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegativeCsvData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msensorid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtimestamp_full\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_timestamp_full\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alldata_dict' is not defined"
     ]
    }
   ],
   "source": [
    "totalNegativeEmbedding = np.empty([1,128], dtype=int)\n",
    "for sensorid in negativeCsvData.keys():\n",
    "    for timestamp in negativeCsvData[sensorid]:\n",
    "        if sensorid in alldata_dict.keys():\n",
    "            all_timestamp_full = negativeCsvData[sensorid]\n",
    "            for timestamp_full in all_timestamp_full:\n",
    "                timestamp = timestamp_full.split('_')[0]\n",
    "                embedding_index = timestamp_full.split('_')[1]\n",
    "                if timestamp in alldata_dict[sensorid]:\n",
    "                    one_negative_embedding = alldata_dict[sensorid][timestamp][int(embedding_index)]\n",
    "                    totalNegativeEmbedding = np.vstack((totalNegativeEmbedding, one_negative_embedding))\n",
    "            \n",
    "totalNegativeEmbedding =np.delete(totalNegativeEmbedding, 0, 0) # remove the first dummy 128 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3980, 128)\n",
      "[[146  14 129 ...   0  62 255]\n",
      " [154  30 171 ...  79 158 255]\n",
      " [153  25 150 ... 114 150 255]\n",
      " ...\n",
      " [164  33 165 ... 211 244 255]\n",
      " [164  37 180 ... 174 166 255]\n",
      " [153  28 167 ...  76 108 255]]\n"
     ]
    }
   ],
   "source": [
    "print(totalNegativeEmbedding.shape)\n",
    "print(totalNegativeEmbedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Add negative labels (0) to the negative embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the negative label, 0\n",
    "negative_xy = np.insert(totalNegativeEmbedding, 128, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3980, 129)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Save the negative embedding and labels to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"negative_xy.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(negative_xy, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Load the negative_xy pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with (open(\"../negative_xy.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            negative_xy= pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_xy_new = np.insert(np.delete(negative_xy, -1, axis=1), 128, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# negative_xy_new\n",
    "\n",
    "with open(\"negative_xy.pickle\", 'wb') as pfile:\n",
    "    pickle.dump(negative_xy_new, pfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Summary:\n",
    "Now we have the following datasets that have been cleaned and formatted to pickles for better usage:  \n",
    "1. ``features.pickle``\n",
    "2. ``positive_xy.pickle``\n",
    "3. ``negative_xy.pickle``\n",
    "\n",
    "The original raw data files are:\n",
    "1. ``features.h5``\n",
    "2. ``positive_samples.pickle``\n",
    "3. ``Sensor_data_negative.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = np.array([1,3,2,4,7,9,0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 7, 3, 1, 2, 0, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty_index = np.argsort(s)[::-1]\n",
    "uncertainty_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unlabeled_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-708065ea365b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muncertainty_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muncertainty_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncertainty_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unlabeled_indices' is not defined"
     ]
    }
   ],
   "source": [
    "uncertainty_index = uncertainty_index[np.in1d(uncertainty_index, unlabeled_indices)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
